{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70450072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Dict\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import seq_aligner\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808da7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_TOKEN = \"<replace with your token>\"\n",
    "LOW_RESOURCE = False\n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN\n",
    ").to(device)\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd7efe",
   "metadata": {},
   "source": [
    "# from p2p_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ceaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_under_image(\n",
    "    image: np.ndarray, text: str, text_color: Tuple[int, int, int] = (0, 0, 0)\n",
    "):\n",
    "    h, w, c = image.shape\n",
    "    offset = int(h * 0.2)\n",
    "    img = np.ones((h + offset, w, c), dtype=np.uint8) * 255\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf\", font_size)\n",
    "    img[:h] = image\n",
    "    textsize = cv2.getTextSize(text, font, 1, 2)[0]\n",
    "    text_x, text_y = (w - textsize[0]) // 2, h + offset - textsize[1] // 2\n",
    "    cv2.putText(img, text, (text_x, text_y), font, 1, text_color, 2)\n",
    "    return img\n",
    "\n",
    "\n",
    "def view_images(images, num_rows=1, offset_ratio=0.02):\n",
    "    if type(images) is list:\n",
    "        num_empty = len(images) % num_rows\n",
    "    elif images.ndim == 4:\n",
    "        num_empty = images.shape[0] % num_rows\n",
    "    else:\n",
    "        images = [images]\n",
    "        num_empty = 0\n",
    "\n",
    "    empty_images = np.ones(images[0].shape, dtype=np.uint8) * 255\n",
    "    images = [image.astype(np.uint8) for image in images] + [empty_images] * num_empty\n",
    "    num_items = len(images)\n",
    "\n",
    "    h, w, c = images[0].shape\n",
    "    offset = int(h * offset_ratio)\n",
    "    num_cols = num_items // num_rows\n",
    "    image_ = (\n",
    "        np.ones(\n",
    "            (\n",
    "                h * num_rows + offset * (num_rows - 1),\n",
    "                w * num_cols + offset * (num_cols - 1),\n",
    "                3,\n",
    "            ),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        * 255\n",
    "    )\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            image_[\n",
    "                i * (h + offset) : i * (h + offset) + h :,\n",
    "                j * (w + offset) : j * (w + offset) + w,\n",
    "            ] = images[i * num_cols + j]\n",
    "\n",
    "    pil_img = Image.fromarray(image_)\n",
    "    display(pil_img)\n",
    "\n",
    "\n",
    "def get_word_inds(text: str, word_place: int, tokenizer):\n",
    "    split_text = text.split(\" \")\n",
    "    if type(word_place) is str:\n",
    "        word_place = [i for i, word in enumerate(split_text) if word_place == word]\n",
    "    elif type(word_place) is int:\n",
    "        word_place = [word_place]\n",
    "    out = []\n",
    "    if len(word_place) > 0:\n",
    "        words_encode = [\n",
    "            tokenizer.decode([item]).strip(\"#\") for item in tokenizer.encode(text)\n",
    "        ][1:-1]\n",
    "        cur_len, ptr = 0, 0\n",
    "\n",
    "        for i in range(len(words_encode)):\n",
    "            cur_len += len(words_encode[i])\n",
    "            if ptr in word_place:\n",
    "                out.append(i + 1)\n",
    "            if cur_len >= len(split_text[ptr]):\n",
    "                ptr += 1\n",
    "                cur_len = 0\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def update_alpha_time_word(\n",
    "    alpha,\n",
    "    bounds: Union[float, Tuple[float, float]],\n",
    "    prompt_ind: int,\n",
    "    word_inds: Optional[torch.Tensor] = None,\n",
    "):\n",
    "    if type(bounds) is float:\n",
    "        bounds = 0, bounds\n",
    "    start, end = int(bounds[0] * alpha.shape[0]), int(bounds[1] * alpha.shape[0])\n",
    "    if word_inds is None:\n",
    "        word_inds = torch.arange(alpha.shape[2])\n",
    "    alpha[:start, prompt_ind, word_inds] = 0\n",
    "    alpha[start:end, prompt_ind, word_inds] = 1\n",
    "    alpha[end:, prompt_ind, word_inds] = 0\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def get_time_words_attention_alpha(\n",
    "    prompts,\n",
    "    num_steps,\n",
    "    cross_replace_steps: Union[float, Dict[str, Tuple[float, float]]],\n",
    "    tokenizer,\n",
    "    max_num_words=77,\n",
    "):\n",
    "    if type(cross_replace_steps) is not dict:\n",
    "        cross_replace_steps = {\"default_\": cross_replace_steps}\n",
    "    if \"default_\" not in cross_replace_steps:\n",
    "        cross_replace_steps[\"default_\"] = (0.0, 1.0)\n",
    "    alpha_time_words = torch.zeros(num_steps + 1, len(prompts) - 1, max_num_words)\n",
    "    for i in range(len(prompts) - 1):\n",
    "        alpha_time_words = update_alpha_time_word(\n",
    "            alpha_time_words, cross_replace_steps[\"default_\"], i\n",
    "        )\n",
    "    for key, item in cross_replace_steps.items():\n",
    "        if key != \"default_\":\n",
    "            inds = [\n",
    "                get_word_inds(prompts[i], key, tokenizer)\n",
    "                for i in range(1, len(prompts))\n",
    "            ]\n",
    "            for i, ind in enumerate(inds):\n",
    "                if len(ind) > 0:\n",
    "                    alpha_time_words = update_alpha_time_word(\n",
    "                        alpha_time_words, item, i, ind\n",
    "                    )\n",
    "    alpha_time_words = alpha_time_words.reshape(\n",
    "        num_steps + 1, len(prompts) - 1, 1, 1, max_num_words\n",
    "    )\n",
    "    return alpha_time_words\n",
    "\n",
    "\n",
    "def latent2image(vae, latents):\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    image = vae.decode(latents)[\"sample\"]\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "def init_latent(latent, model, height, width, generator, batch_size):\n",
    "    if latent is None:\n",
    "        latent = torch.randn(\n",
    "            (1, model.unet.in_channels, height // 8, width // 8),\n",
    "            generator=generator,\n",
    "        )\n",
    "    latents = latent.expand(\n",
    "        batch_size, model.unet.in_channels, height // 8, width // 8\n",
    "    ).to(model.device)\n",
    "    return latent, latents\n",
    "\n",
    "\n",
    "def diffusion_step(\n",
    "    model, controller, latents, context, t, guidance_scale, low_resource=False\n",
    "):\n",
    "    if low_resource:\n",
    "        noise_pred_uncond = model.unet(latents, t, encoder_hidden_states=context[0])[\n",
    "            \"sample\"\n",
    "        ]\n",
    "        noise_prediction_text = model.unet(\n",
    "            latents, t, encoder_hidden_states=context[1]\n",
    "        )[\"sample\"]\n",
    "    else:\n",
    "        latents_input = torch.cat([latents] * 2)\n",
    "        noise_pred = model.unet(latents_input, t, encoder_hidden_states=context)[\n",
    "            \"sample\"\n",
    "        ]\n",
    "        noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "        \n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "        noise_prediction_text - noise_pred_uncond\n",
    "    )\n",
    "    latents = model.scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
    "    latents = controller.step_callback(latents)\n",
    "    return latents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fee359",
   "metadata": {},
   "source": [
    "# attention map edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [\n",
    "            item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS)\n",
    "            for item in maps\n",
    "        ]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 + 1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.threshold)\n",
    "        mask = (mask[:1] + mask[1:]).float()\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "\n",
    "    def __init__(self, prompts: List[str], words: List[List[str]], threshold=0.3):\n",
    "        alpha_layers = torch.zeros(len(prompts), 1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                h = attn.shape[0]\n",
    "                attn[h // 2 :] = self.forward(attn[h // 2 :], is_cross, place_in_unet)\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    "\n",
    "\n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\n",
    "            \"down_cross\": [],\n",
    "            \"mid_cross\": [],\n",
    "            \"up_cross\": [],\n",
    "            \"down_self\": [],\n",
    "            \"mid_self\": [],\n",
    "            \"up_self\": [],\n",
    "        }\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32**2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {\n",
    "            key: [item / self.cur_step for item in self.attention_store[key]]\n",
    "            for key in self.attention_store\n",
    "        }\n",
    "        return average_attention\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "\n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    #todo 先不用管，AttentionRefine可以用不上这个blend\n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "\n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16**2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (\n",
    "            self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]\n",
    "        ):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
    "                attn_repalce_new = (\n",
    "                    self.replace_cross_attention(attn_base, attn_repalce) * alpha_words\n",
    "                    + (1 - alpha_words) * attn_repalce\n",
    "                )\n",
    "                attn[1:] = attn_repalce_new\n",
    "            else:\n",
    "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: Union[\n",
    "            float, Tuple[float, float], Dict[str, Tuple[float, float]]\n",
    "        ],\n",
    "        self_replace_steps: Union[float, Tuple[float, float]],\n",
    "        local_blend: Optional[LocalBlend],\n",
    "    ):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = get_time_words_attention_alpha(\n",
    "            prompts, num_steps, cross_replace_steps, tokenizer\n",
    "        ).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(\n",
    "            num_steps * self_replace_steps[1]\n",
    "        )\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "\n",
    "# todo\n",
    "# 应该是只需要用上这个replace\n",
    "class AttentionReplace(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        return torch.einsum(\"hpw,bwn->bhpn\", attn_base, self.mapper)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "    ):\n",
    "        super(AttentionReplace, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
    "\n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "    ):\n",
    "        super(AttentionRefine, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "class AttentionReweight(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        if self.prev_controller is not None:\n",
    "            attn_base = self.prev_controller.replace_cross_attention(\n",
    "                attn_base, att_replace\n",
    "            )\n",
    "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompts,\n",
    "        num_steps: int,\n",
    "        cross_replace_steps: float,\n",
    "        self_replace_steps: float,\n",
    "        equalizer,\n",
    "        local_blend: Optional[LocalBlend] = None,\n",
    "        controller: Optional[AttentionControlEdit] = None,\n",
    "    ):\n",
    "        super(AttentionReweight, self).__init__(\n",
    "            prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend\n",
    "        )\n",
    "        self.equalizer = equalizer.to(device)\n",
    "        self.prev_controller = controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7bc2f",
   "metadata": {},
   "source": [
    "## image generate pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ca1c5",
   "metadata": {},
   "source": [
    "### forward改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def register_attention_control(model, controller):\n",
    "    def ca_forward(self, place_in_unet):\n",
    "        to_out = self.to_out\n",
    "        if type(to_out) is torch.nn.modules.container.ModuleList:\n",
    "            to_out = self.to_out[0]\n",
    "        else:\n",
    "            to_out = self.to_out\n",
    "        #todo 这里好像是原本的注意力实现，只不过为了加入controller需要搬过来\n",
    "        def forward(x, context=None, mask=None):\n",
    "            batch_size, sequence_length, dim = x.shape\n",
    "            h = self.heads\n",
    "            q = self.to_q(x)\n",
    "            is_cross = context is not None\n",
    "            context = context if is_cross else x\n",
    "            k = self.to_k(context)\n",
    "            v = self.to_v(context)\n",
    "            q = self.reshape_heads_to_batch_dim(q)\n",
    "            k = self.reshape_heads_to_batch_dim(k)\n",
    "            v = self.reshape_heads_to_batch_dim(v)\n",
    "\n",
    "            sim = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = mask.reshape(batch_size, -1)\n",
    "                max_neg_value = -torch.finfo(sim.dtype).max\n",
    "                mask = mask[:, None, :].repeat(h, 1, 1)\n",
    "                sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "            # attention, what we cannot get enough of\n",
    "            attn = sim.softmax(dim=-1)\n",
    "            \n",
    "            #todo 这里通过controller进行控制\n",
    "            attn = controller(attn, is_cross, place_in_unet)\n",
    "            out = torch.einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "            out = self.reshape_batch_dim_to_heads(out)\n",
    "            return to_out(out)\n",
    "\n",
    "        return forward\n",
    "\n",
    "    class DummyController:\n",
    "\n",
    "        def __call__(self, *args):\n",
    "            return args[0]\n",
    "\n",
    "        def __init__(self):\n",
    "            self.num_att_layers = 0\n",
    "\n",
    "    if controller is None:\n",
    "        controller = DummyController()\n",
    "\n",
    "    def register_recr(net_, count, place_in_unet):\n",
    "        if net_.__class__.__name__ == \"CrossAttention\":\n",
    "            net_.forward = ca_forward(net_, place_in_unet)\n",
    "            return count + 1\n",
    "        elif hasattr(net_, \"children\"):\n",
    "            for net__ in net_.children():\n",
    "                count = register_recr(net__, count, place_in_unet)\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    sub_nets = model.unet.named_children()\n",
    "    for net in sub_nets:\n",
    "        if \"down\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"down\")\n",
    "        elif \"up\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"up\")\n",
    "        elif \"mid\" in net[0]:\n",
    "            cross_att_count += register_recr(net[1], 0, \"mid\")\n",
    "\n",
    "    controller.num_att_layers = cross_att_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea28bd",
   "metadata": {},
   "source": [
    "### 具体图像生成流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a905ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def text2image_ldm_stable(\n",
    "    model,\n",
    "    prompt: List[str],\n",
    "    controller,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: float = 7.5,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    latent: Optional[torch.FloatTensor] = None,\n",
    "    low_resource: bool = False,\n",
    "):\n",
    "    register_attention_control(model, controller)\n",
    "    height = width = 512\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "    text_input = model.tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_embeddings = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = model.tokenizer(\n",
    "        [\"\"] * batch_size,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    uncond_embeddings = model.text_encoder(uncond_input.input_ids.to(model.device))[0]\n",
    "\n",
    "    context = [uncond_embeddings, text_embeddings]\n",
    "    if not low_resource:\n",
    "        context = torch.cat(context)\n",
    "        \n",
    "    latent, latents = init_latent(latent, model, height, width, generator, batch_size)\n",
    "\n",
    "    # set timesteps\n",
    "    extra_set_kwargs = {\"offset\": 1}\n",
    "    model.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
    "    for t in tqdm(model.scheduler.timesteps):\n",
    "        latents = diffusion_step(\n",
    "            model, controller, latents, context, t, guidance_scale, low_resource\n",
    "        )\n",
    "\n",
    "    image = latent2image(model.vae, latents)\n",
    "\n",
    "    return image, latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693de09",
   "metadata": {},
   "source": [
    "### 可视化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_attention(\n",
    "    attention_store: AttentionStore,\n",
    "    res: int,\n",
    "    from_where: List[str],\n",
    "    is_cross: bool,\n",
    "    select: int,\n",
    "):\n",
    "    out = []\n",
    "    attention_maps = attention_store.get_average_attention()\n",
    "    num_pixels = res**2\n",
    "    for location in from_where:\n",
    "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
    "            if item.shape[1] == num_pixels:\n",
    "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[\n",
    "                    select\n",
    "                ]\n",
    "                out.append(cross_maps)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    out = out.sum(0) / out.shape[0]\n",
    "    return out.cpu()\n",
    "\n",
    "\n",
    "def show_cross_attention(\n",
    "    attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0\n",
    "):\n",
    "    tokens = tokenizer.encode(prompts[select])\n",
    "    decoder = tokenizer.decode\n",
    "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
    "    images = []\n",
    "    for i in range(len(tokens)):\n",
    "        image = attention_maps[:, :, i]\n",
    "        image = 255 * image / image.max()\n",
    "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
    "        image = image.numpy().astype(np.uint8)\n",
    "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
    "        image = text_under_image(image, decoder(int(tokens[i])))\n",
    "        images.append(image)\n",
    "    view_images(np.stack(images, axis=0))\n",
    "\n",
    "\n",
    "def show_self_attention_comp(\n",
    "    attention_store: AttentionStore,\n",
    "    res: int,\n",
    "    from_where: List[str],\n",
    "    max_com=10,\n",
    "    select: int = 0,\n",
    "):\n",
    "    attention_maps = (\n",
    "        aggregate_attention(attention_store, res, from_where, False, select)\n",
    "        .numpy()\n",
    "        .reshape((res**2, res**2))\n",
    "    )\n",
    "    u, s, vh = np.linalg.svd(\n",
    "        attention_maps - np.mean(attention_maps, axis=1, keepdims=True)\n",
    "    )\n",
    "    images = []\n",
    "    for i in range(max_com):\n",
    "        image = vh[i].reshape(res, res)\n",
    "        image = image - image.min()\n",
    "        image = 255 * image / image.max()\n",
    "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
    "        image = Image.fromarray(image).resize((256, 256))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    view_images(np.concatenate(images, axis=1))\n",
    "\n",
    "\n",
    "def run_and_display(\n",
    "    prompts, controller, latent=None, run_baseline=False, generator=None\n",
    "):\n",
    "    if run_baseline:\n",
    "        print(\"w.o. prompt-to-prompt\")\n",
    "        images, latent = run_and_display(\n",
    "            prompts,\n",
    "            EmptyControl(),\n",
    "            latent=latent,\n",
    "            run_baseline=False,\n",
    "            generator=generator,\n",
    "        )\n",
    "        print(\"with prompt-to-prompt\")\n",
    "    images, x_t = text2image_ldm_stable(\n",
    "        ldm_stable,\n",
    "        prompts,\n",
    "        controller,\n",
    "        latent=latent,\n",
    "        num_inference_steps=NUM_DIFFUSION_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        generator=generator,\n",
    "        low_resource=LOW_RESOURCE,\n",
    "    )\n",
    "    view_images(images)\n",
    "    return images, x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083f93f",
   "metadata": {},
   "source": [
    "# examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"a photo of a house on a mountain\",\n",
    "    \"a photo of a house on a mountain at fall\",\n",
    "]\n",
    "\n",
    "\n",
    "controller = AttentionRefine(\n",
    "    prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=0.8, self_replace_steps=0.4\n",
    ")\n",
    "_ = run_and_display(prompts, controller, latent=x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ecf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ac4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c8cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8affd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15ff6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d3308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4e37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b6de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eee6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebb4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06e5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a67540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
